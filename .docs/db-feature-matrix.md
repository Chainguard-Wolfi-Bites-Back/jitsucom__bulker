# Database Feature Matrix

## Glossary

* üöø**Stream** ‚Äî a mode when Bulker inserts data to a destination on per record basis. Usually,
databases don't like when a large amount of data is streamed. Don't use at production scale (more than 10-100 records per minute, 
depending on database).
* üõ¢Ô∏è**Batch** ‚Äî a mode when Bulker inserts data to a destination in batches. Preferred mode for large amounts of data.
* üîë**Primary Key** - a primary key is the column or columns that contain values that uniquely identify each row in a table. Enabled via stream options. Required for 'deduplication' option.
* üê´**Deduplication** ‚Äî a mode that avoid duplication of data rows with the equal values of key columns (primary key). It means that if Bulker receives
a record with the same primary key values, the old one will be replaced. Bulker maintains uniqueness of rows based on primary key columns even for warehouses that doesn't enforce uniqueness natively. Enabled via stream options. Require primary key option.
May comes with performance tradeoffs.


### Advanced features

Those features are not exposed as HTTP API and supported only on Go-lib API level.

* **Replace Table** - a special version of batch mode that assumes that a single batch contains all data for a table. Depending on database implementation bulker tries to atomically replace old table with a new one.
* **Replace Partition** - a special version of batch mode that replaces a part of target table. Part of table to replace is defined by 'partition' stream option. Each batch loads data for virtual partition identified by 'partition' option value. If table already contains data for provided 'partition', this data will be deleted and replaced with new data from current batch. Enabled via stream options.

|                        | Redshift                                   | BigQuery                                                    | ClickHouse                                                            | Snowflake                                   | Postgres                                   | MySQL                                   | S3 (coming soon) |     |
|------------------------|--------------------------------------------|-------------------------------------------------------------|-----------------------------------------------------------------------|---------------------------------------------|--------------------------------------------|-----------------------------------------|------------------|-----|
| Stream                 | ‚úÖ [Supported](#redshift-stream)<br/>‚ö†Ô∏èSlow | ‚ùå [Not supported](#bigquery-stream)                         | ‚úÖ [Supported](#clickhouse-stream)                                     | ‚úÖ [Supported](#snowflake-stream)            | ‚úÖ [Supported](#postgres-stream)            | ‚úÖ [Supported](#mysql-stream)            |                  |     |
| Batch                  | ‚úÖ [Supported](#redshift-batch)             | ‚úÖ [Supported](#bigquery-batch)                              | ‚úÖ [Supported](#clickhouse-batch)                                      | ‚úÖ [Supported](#snowflake-batch)             | ‚úÖ [Supported](#postgres-batch)             | ‚úÖ [Supported](#mysql-batch)             |                  |     |
| Deduplication          | ‚úÖ [Supported](#redshift-deduplication)     | ‚úÖ [Supported](#bigquery-deduplication)                      | ‚úÖ [Supported](#clickhouse-deduplication)<br/>‚ö†Ô∏èEventual deduplication | ‚úÖ [Supported](#snowflake-deduplication)     | ‚úÖ [Supported](#postgres-deduplication)     | ‚úÖ [Supported](#mysql-deduplication)     |                  |     |
| Primary key            | ‚úÖ [Supported](#redshift-primary-key)       | ‚ÑπÔ∏è [Emulated](#bigquery-primary-key)                        | ‚úÖÔ∏è [Supported](#clickhouse-primary-key)                               | ‚úÖÔ∏è [Supported](#snowflake-primary-key)      | ‚úÖÔ∏è [Supported](#postgres-primary-key)      | ‚úÖÔ∏è [Supported](#mysql-primary-key)      |                  |     |
| **Advanced features:** |                                            |                                                             |                                                                       |                                             |                                            |                                         |                  |     |
| Replace Table          | ‚úÖ [Supported](#redshift-replace-table)     | ‚úÖ [Supported](#bigquery-replace-table)                      | ‚úÖ [Supported](#clickhouse-replace-table)                              | ‚úÖ [Supported](#snowflake-replace-table)     | ‚úÖ [Supported](#postgres-replace-table)     | ‚úÖ [Supported](#mysql-replace-table)     |                  |     |
| Replace Partition      | ‚úÖ [Supported](#redshift-replace-partition) | ‚úÖ [Supported](#bigquery-replace-partition)<br/>‚ö†Ô∏èNot atomic | ‚úÖ [Supported](#clickhouse-replace-partition)                          | ‚úÖ [Supported](#snowflake-replace-partition) | ‚úÖ [Supported](#postgres-replace-partition) | ‚úÖ [Supported](#mysql-replace-partition) |                  |     |



## Redshift

### Redshift Stream

‚úÖSupported

‚ö†Ô∏èPerformance considerations

Supported as plain insert statements. Don't use at production scale (more than 10 records per minute)

### Redshift Batch

‚úÖSupported

Algorithm:

- Write to tmp file
- Load tmp file to s3
- `BEGIN TRANSACTION`
- `COPY from s3 to tmp_table`
- `INSERT into target_table select from tmp_table`
- `COMMIT`

### Redshift Deduplication

‚úÖSupported

For batch mode the following algorithm is used:

- Write to tmp file
- Deduplicate rows in tmp file
- Load tmp file to s3
- `BEGIN TRANSACTION`
- `COPY from s3 to tmp_table`
- `DELETE from target_table using tmp_table` where primary key matches
- `INSERT into target_table select from tmp_table`
- `COMMIT`

For stream mode:

`SELECT` by primary key. Then either `INSERT` or `UPDATE` depending on result. Don't use at production scale (more than 10 records per minute)

### Redshift Primary Key

‚úÖSupported

In Redshift primary keys doesn‚Äôt enforce uniqueness.
Bulker performs deduplication itself when deduplication option is enabled and primary key is specified.

If primary key consists of a single column, that column will also be selected as the `DIST KEY`.

### Redshift Replace Table

‚úÖSupported

Algorithm:
- Write to tmp file
- Load tmp file to s3
- `BEGIN TRANSACTION`
- `COPY from s3 to tmp_table`
- `RENAME target_table to deprecated_target_table_20060101_150405`
- `RENAME tmp_table to target_table`
- `DROP TABLE deprecated_target_table_20060101_150405`
- `COMMIT`

### Redshift Replace Partition

‚úÖSupported

Algorithm:
- Write to tmp file
- Load tmp file to s3
- `BEGIN TRANSACTION`
- `DELETE from target_table where partition_id=partiton option value`
- `COPY from s3 to target_table`
- `COMMIT`

## BigQuery

### BigQuery Stream

‚ùå Not supported, though it's possible to implement.

### BigQuery Batch

‚úÖSupported

- Write to tmp file
- Use Loader API to load to tmp_table from tmp file
- Use Copier API to copy from tmp_table to target_table

### BigQuery Deduplication

‚úÖSupported

Algorithm for batch mode:
- Write to tmp file
- Dedup tmp file
- Use Loader API to load to tmp_table from tmp file
- `MERGE into target_table on tmp_table when matched then UPDATE when not matched them INSERT`

### BigQuery Primary Key

Emulated - bulker fully handles uniqueness.
Primary keys columns meta information stored in table labels.

### BigQuery Replace Table

‚úÖSupported

Algorithm:
- Write to tmp file
- Use Loader API to load to tmp_table from tmp file
- Use Copier API to copy from tmp_table to target_table with WriteTruncate mode
- Drop tmp_table

### BigQuery Replace Partition

‚úÖSupported
‚ö†Ô∏èNot atomic ‚Äì during completion of bulker stream it is possible that target table will be missing some data for specified 'partiton' for a short period of time.

Algorithm:
- `DELETE from target_table where partition_id=` partition option value
- Write to tmp file
- Use Loader API to load to target_table from tmp file

## ClickHouse

### ClickHouse Stream

‚úÖSupported

For single node instance:

`INSERT INTO target_table (...) VALUES (..)`

For cluster bulker insert into distributed table so data evenly distributed across cluster nodes:

`INSERT INTO dist_target_table (...) VALUES (...)`

### ClickHouse Batch

‚úÖSupported

Algorithm:
- Write to tmp file
- `INSERT INTO tmp_table (...) VALUES (...)` - bulk load data from tmp file into tmp_table using prepared statement in transaction
- `INSERT INTO target_table(...) SELECT ... FROM tmp_table`

### ClickHouse Deduplication

‚úÖSupported 

Bulker clickhouse implementation relies on clickhouse [ReplacingMergeTree](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replacingmergetree/)
engine to perform deduplication.
Primary key columns are used as primary key as well as sorting keys (`ORDER BY`) for ReplacingMergeTree engine.

‚ö†Ô∏èEventual deduplication

ReplacingMergeTree engine performs deduplication in background during some time after insertion
So it's still possible to get rows with duplicated primary key columns using ordinary `SELECT`.

To make sure that no duplicates are present in query results use [FINAL](https://clickhouse.com/docs/en/sql-reference/statements/select/from/#final-modifier) modifier, e.g:

`SELECT * FROM target_table FINAL`.

### ClickHouse Primary Key

‚úÖSupported

Primary keys columns also used as sorting key for ReplacingMergeTree engine.

### ClickHouse Replace Table

‚úÖSupported

Algorithm:
- Write to tmp file
- `INSERT INTO tmp_table (...) VALUES (...)` - bulk load data from tmp file into tmp_table using prepared statement in transaction
- `EXCHANGE TABLES target_table tmp_table`

### ClickHouse Replace Partition

‚úÖSupported

Algorithm:
- Write to tmp file
- `INSERT INTO tmp_table(...) VALUES (...)` - bulk load data from tmp file into tmp_table using prepared statement in transaction
- `INSERT INTO target_table(...) SELECT ... FROM tmp_table`


## Snowflake

### Snowflake Stream

‚úÖSupported

`INSERT INTO target_table (...) VALUES (..)`

### Snowflake Batch

‚úÖSupported

Algorithm:

- Write to tmp file
- Load tmp file to `stage`
- `BEGIN TRANSACTION`
- `COPY from stage to tmp_table`
- `INSERT into target_table select from tmp_table`
- `COMMIT`

### Snowflake Deduplication

‚úÖSupported

For batch mode the following algorithm is used:

- Write to tmp file
- Deduplicate rows in tmp file
- Load tmp file to s3
- `BEGIN TRANSACTION`
- `COPY from stage to tmp_table`
- `MERGE into target_table using (select from tmp_table) ...`
- `COMMIT`

For stream mode:

`SELECT` by primary key. Then either `INSERT` or `UPDATE` depending on result.

### Snowflake Primary Key

‚úÖSupported

In Snowflake primary keys doesn‚Äôt enforce uniqueness.
Bulker performs deduplication itself when deduplication option is enabled and primary key is specified.

### Snowflake Replace Table

‚úÖSupported

Algorithm:
- Write to tmp file
- Load tmp file to `stage`
- `BEGIN TRANSACTION`
- `COPY from stage to tmp_table`
- `RENAME target_table to deprecated_target_table_20060101_150405`
- `RENAME tmp_table to target_table`
- `DROP TABLE deprecated_target_table_20060101_150405`
- `COMMIT`

### Snowflake Replace Partition

‚úÖSupported

Algorithm:
- Write to tmp file
- Load tmp file to `stage`
- `BEGIN TRANSACTION`
- `DELETE from target_table where partition_id=partiton option value`
- `COPY from stage to target_table`
- `COMMIT`

## Postgres

### Postgres Stream

‚úÖSupported

`INSERT INTO target_table (...) VALUES (..)`

### Postgres Batch

‚úÖSupported

Algorithm:

- Write to tmp file
- `BEGIN TRANSACTION`
- `COPY from STDIN to tmp_table` - load tmp file into tmp_table
- `INSERT into target_table select from tmp_table`
- `COMMIT`

### Postgres Deduplication

‚úÖSupported

For batch mode the following algorithm is used:

- Write to tmp file
- Deduplicate rows in tmp file
- `BEGIN TRANSACTION`
- `COPY from STDIN to tmp_table` - load tmp file into tmp_table
- `INSERT into target_table select from tmp_table ON CONFLICT UPDATE ...`
- `COMMIT`

For stream mode:

`INSERT INTO target_table (...) VALUES (..) ON CONFLICT UPDATE ...`

### Postgres Primary Key

‚úÖSupported

### Postgres Replace Table

‚úÖSupported

Algorithm:
- Write to tmp file
- `BEGIN TRANSACTION`
- `COPY from STDIN to tmp_table` - load tmp file into tmp_table
- `RENAME target_table to deprecated_target_table_20060101_150405`
- `RENAME tmp_table to target_table`
- `DROP TABLE deprecated_target_table_20060101_150405`
- `COMMIT`

### Postgres Replace Partition

‚úÖSupported

Algorithm:
- Write to tmp file
- `BEGIN TRANSACTION`
- `DELETE from target_table where partition_id=partiton option value`
- `COPY from STDIN to target_table` - load tmp file into tmp_table
- `COMMIT`

## MySQL

### MySQL Stream

‚úÖSupported

`INSERT INTO target_table (...) VALUES (..)`

### MySQL Batch

‚úÖSupported

Algorithm:

- `BEGIN TRANSACTION`
- `INSERT into tmp_table`
- `INSERT into target_table select from tmp_table`
- `COMMIT`

### MySQL Deduplication

‚úÖSupported

For batch mode the following algorithm is used:

- `BEGIN TRANSACTION`
- `INSERT into tmp_table ... ON DUPLICATE KEY UPDATE ...`
- `INSERT into target_table select from tmp_table ... ON DUPLICATE KEY UPDATE ...`
- `COMMIT`

For stream mode:

`INSERT INTO target_table ... ON DUPLICATE KEY UPDATE ...`

### MySQL Primary Key

‚úÖSupported


### MySQL Replace Table

‚úÖSupported

Algorithm:
- `BEGIN TRANSACTION`
- `INSERT into tmp_table`
- `RENAME target_table to deprecated_target_table_20060101_150405`
- `RENAME tmp_table to target_table`
- `DROP TABLE deprecated_target_table_20060101_150405`
- `COMMIT`

### MySQL Replace Partition

‚úÖSupported

Algorithm:
- `BEGIN TRANSACTION`
- `DELETE from target_table where partition_id=partiton option value`
- `INSERT into target_table ... ON DUPLICATE KEY UPDATE ...`
- `COMMIT`